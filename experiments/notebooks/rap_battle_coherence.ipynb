{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bbcb21450742c9a80eab260f7a512b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eron/miniconda3/envs/cuda_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/eron/miniconda3/envs/cuda_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8b4494f770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import bitsandbytes\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, AutoModelForCausalLM\n",
    "sys.path.insert(0, '../..')\n",
    "import drugs_conf as conf\n",
    "current_script_dir = os.getcwd()\n",
    "results_dir = os.path.join(current_script_dir, '..', 'results')\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "streamer = TextStreamer(tokenizer)\n",
    "sober_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True)\n",
    "sober_model.eval()\n",
    "torch.manual_seed(420)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sober content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]\n",
      "\n",
      "Write an epic rap battle between William Rowe Hamilton and Lord Kelvin[/INST]\n",
      "\n",
      "[Scene: A dark and dimly lit underground rap club, the air thick with anticipation. The crowd is on the edge of their seats as two of the greatest minds in science history take the stage. William Rowe Hamilton and Lord Kelvin are about to engage in an epic rap battle.]\n",
      "\n",
      "William Rowe Hamilton:\n",
      "Yo, I'm the king of the math game,\n",
      "With equations that'll make your head spin like a flame,\n",
      "I'm the one who brought you quaternions,\n",
      "And I'll leave you in the dust, like a poor excuse for a reason.\n",
      "\n",
      "Lord Kelvin:\n",
      "Hold up, Hamilton, you ain't ready,\n",
      "I'm the one who's got the science, the facts, and the\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.load(os.path.join(results_dir, 'vergence', 'start_tensor.pt'))\n",
    "print(tokenizer.decode(input_ids[0]))\n",
    "with torch.no_grad():\n",
    "    sober_outputs = sober_model(input_ids=input_ids, past_key_values=None, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drugs.dgenerate import DRUGS, print_top_k_logits_histogram, set_index_base\n",
    "\n",
    "drugs = DRUGS()\n",
    "def run_test(model, drug_type, base_name, sub_name, seed, dose_shape_callback=None, layer_begin=2, layer_end=31, dose_start=0, dose_end=3.14):\n",
    "    with torch.no_grad():\n",
    "        noise_stop_layer = layer_begin\n",
    "        microdose= dose_start\n",
    "        while noise_stop_layer < layer_end:\n",
    "            dose_shape, interp_mode = dose_shape_callback(noise_stop_layer)\n",
    "            drugs._set_typed_dose_shape(dose_shape, interp_mode, drug_type)\n",
    "            while microdose < dose_end:\n",
    "                torch.manual_seed(seed)\n",
    "                drugs.set_dose_theta(microdose, drug_type)\n",
    "                result = model(input_ids=input_ids, past_key_values=None, output_hidden_states=True)\n",
    "                clear_output(wait=True)\n",
    "                print_top_k_logits_histogram(result.logits.to('cpu'), tokenizer, top_k=20, softmax=False)\n",
    "                drugs.save_trace(base_name, sub_name, noise_stop_layer, result.logits, 40, result.hidden_states, tokenizer)            \n",
    "                microdose += 0.05\n",
    "            noise_stop_layer = noise_stop_layer + 1 #if noise_stop_layer < 4 else noise_stop_layer + 4\n",
    "            microdose=0\n",
    "\n",
    "def layerwise_callback(at_layer):\n",
    "    return [{'depth': (at_layer-1.1)/32, 'peakratio': 0},\n",
    "            {'depth': (at_layer-1)/32, 'peakratio': 1},\n",
    "            {'depth': (at_layer+1)/32, 'peakratio' : 1},\n",
    "            {'depth': (at_layer+1.1)/32, 'peakratio' : 0}], 'ceil'\n",
    "\n",
    "def fullstack_calback(at_layer):\n",
    "    return([{'depth': -1, 'peakratio': 1},\n",
    "            {'depth': (at_layer+1.1)/32, 'peakratio': 0}], 'ceil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DRÂµGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Drugs] Injected drugs into 32 attention classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "\n",
      "\u001b[38;5;247m                                                                      duty  6.75%\u001b[0m \n",
      "\u001b[38;5;248m                                                                     logic  6.86%\u001b[0m \n",
      "\u001b[38;5;249m                                                                   feature  6.91%\u001b[0m \n",
      "\u001b[38;5;250m                                                                    steady  7.24%\u001b[0m \n",
      "\u001b[38;5;251m                                                                technology  8.14%\u001b[0m \n",
      "\u001b[38;5;252m                                                                      cre  12.81%\u001b[0m \n",
      "\u001b[38;5;253m                                                                knowledge  15.46%\u001b[0m \n",
      "\u001b[38;5;254m####                                                                 cred  32.98%\u001b[0m \n",
      "\u001b[38;5;255m######                                                                dec  35.66%\u001b[0m \n",
      "\u001b[38;5;16m#########################################################         energy  100.00%\u001b[0m \n",
      "---::::energy:::   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eron/drugs/experiments/notebooks/../../drugs/dgenerate.py:661: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probabilities = F.softmax(torch.tensor(last_logits), dim=-1)\n",
      "/home/eron/drugs/experiments/notebooks/../../drugs/dgenerate.py:667: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  top_k_res = F.softmax(torch.tensor(last_logits), dim=-1)[top_k_indices].numpy() if softmax else last_logits[top_k_indices]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "model = drugs.inject(sober_model, tokenizer=tokenizer)\n",
    "set_index_base(sober_outputs.logits.to('cpu'))\n",
    "print_top_k_logits_histogram(sober_outputs.logits[:,:,:].to('cpu'), tokenizer=tokenizer)\n",
    "microdose=0\n",
    "noise_stop_layer = 2\n",
    "seed = 420\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(model, 'A', f\"Hamilton_A_dose\", \"single_layer\",  seed, layerwise_callback)\n",
    "drugs.set_A_dose_theta(0)\n",
    "run_test(model, 'Q', f\"Hamilton_Q_dose\", \"single_layer\", seed, layerwise_callback)\n",
    "drugs.set_Q_dose_theta(0)\n",
    "run_test(model, 'K', f\"Hamilton_K_dose\", \"single_layer\", seed, layerwise_callback)\n",
    "drugs.set_K_dose_theta(0)\n",
    "run_test(model, 'V', f\"Hamilton_V_dose\", \"single_layer\", seed, layerwise_callback)\n",
    "drugs.set_V_dose_theta(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "\n",
      "\u001b[38;5;16m###########################################                           lea  76.35%\u001b[0m \n",
      "\u001b[38;5;16m###########################################                           Ã©ch  76.40%\u001b[0m \n",
      "\u001b[38;5;16m###########################################                           Naz  76.40%\u001b[0m \n",
      "\u001b[38;5;16m###########################################                          rola  76.77%\u001b[0m \n",
      "\u001b[38;5;16m###########################################                         ÑÑÐ°Ð»Ñ  77.49%\u001b[0m \n",
      "\u001b[38;5;16m###########################################                          Chem  77.49%\u001b[0m \n",
      "\u001b[38;5;16m############################################                       ermann  77.81%\u001b[0m \n",
      "\u001b[38;5;16m############################################                         onne  78.27%\u001b[0m \n",
      "\u001b[38;5;16m#############################################                      imiter  79.11%\u001b[0m \n",
      "\u001b[38;5;16m###############################################                      agli  82.33%\u001b[0m \n",
      "\u001b[38;5;16m###############################################                     Stein  82.38%\u001b[0m \n",
      "\u001b[38;5;16m################################################                     Stim  83.16%\u001b[0m \n",
      "\u001b[38;5;16m################################################                     vast  83.42%\u001b[0m \n",
      "\u001b[38;5;16m################################################                      cup  83.73%\u001b[0m \n",
      "\u001b[38;5;16m####################################################                  Chi  88.15%\u001b[0m \n",
      "\u001b[38;5;16m#####################################################               fluid  89.76%\u001b[0m \n",
      "\u001b[38;5;16m#####################################################                  of  89.86%\u001b[0m \n",
      "\u001b[38;5;16m######################################################                hre  90.54%\u001b[0m \n",
      "\u001b[38;5;16m########################################################             ilar  93.66%\u001b[0m \n",
      "\u001b[38;5;16m#############################################################       rane  100.00%\u001b[0m \n",
      "---::::rane:::   \n",
      "\n",
      "saving: /home/eron/drugs/drugs/../experiments/results/Hamilton_V_dose/fullstack/injection_depth_30_(0A_0Q_0K_177V)-dose.msgpack\n"
     ]
    }
   ],
   "source": [
    "drugs.set_V_dose_theta(0)\n",
    "run_test(model, 'A', \"Hamilton_A_dose\", \"fullstack\", seed, fullstack_calback)\n",
    "drugs.set_A_dose_theta(0)\n",
    "run_test(model, 'Q', \"Hamilton_Q_dose\", \"fullstack\", seed, fullstack_calback)\n",
    "drugs.set_Q_dose_theta(0)\n",
    "run_test(model, 'K', \"Hamilton_K_dose\", \"fullstack\", seed, fullstack_calback, layer_begin=12, dose_start=(120/180)*3.14)\n",
    "drugs.set_K_dose_theta(0)\n",
    "run_test(model, 'V', \"Hamilton_V_dose\", \"fullstack\", seed, fullstack_calback)\n",
    "drugs.set_V_dose_theta(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
