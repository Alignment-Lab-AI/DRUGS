{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb09dee55e547dcbbb16f90e490e213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eron/miniconda3/envs/cuda_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/eron/miniconda3/envs/cuda_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import bitsandbytes\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, AutoModelForCausalLM\n",
    "from drugs.dgenerate import DRUGS\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "sober_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True)\n",
    "sober_model.eval()\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dosage\n",
    "Range 0 - pi. Where pi is way too much. You can go higher, but don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Drugs] Injected drugs into 32 attention classes.\n"
     ]
    }
   ],
   "source": [
    "drugs = DRUGS()\n",
    "drugs.set_A_dose_theta(0.1) #you can also specify K_dose_theta, V_dose_theta, Q_dose_theta, or any combination of the 4. \n",
    "model = drugs.inject(sober_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRÂµG profile\n",
    "Advanced control. Lets you specify how much various depths of the network are injected with how much noise. You can use 'interpolate' as the mode to smoothly vary between the points you specify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_depth = 0.4 #how deep to shove the needle in\n",
    "spread = 0.1 #how many layers to dose on either side of the injection site\n",
    "\n",
    "drug_profile = ([\n",
    "    {'depth': (injection_depth-(spread*1.01)), 'peakratio': 0},\n",
    "    {'depth': (injection_depth-spread), 'peakratio': 1},\n",
    "    {'depth': (injection_depth+spread), 'peakratio' : 1},\n",
    "    {'depth': (injection_depth+(spread*1.01)), 'peakratio' : 0}], \n",
    "'ceil')\n",
    "drugs.set_A_dose_shape(drug_profile) #each profile (A, K, Q, or V) can be independently injected into different layers, if you are expecially picky about what your noise is doing to which things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "By default this notebook prompts you for input. If viewed in a browser, a dialogue will pop up asking you to say something. \n",
    "\n",
    "Note that all variety in the model's responses is due purely to the noise being injected, the selected token is ALWAYS whatever the model thinks is the most likely one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input = str(input(\"\\bAsk Something:\"))\n",
    "tokenized_start = tokenizer.apply_chat_template([\n",
    "    {'role': 'system',\n",
    "    'content': 'You are Alan Watts.'},\n",
    "    {'role': 'user', \n",
    "     'content': initial_input}\n",
    "], return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        generated_tokens = model.Dgenerate(\n",
    "                    input_ids = tokenized_start,\n",
    "                    streamer = streamer, \n",
    "                )\n",
    "        print(\"\\n\\nAsk Something:\", end=\"\")\n",
    "        model.cold_shower(True) #Sets the kv-cache back to theoretically pure baseline, if this is important to you.\n",
    "        await_input = str(input(\": \"))\n",
    "        tokenized_start = tokenizer.apply_chat_template([{\n",
    "            'role': 'user',\n",
    "            'content': await_input}], return_tensors=\"pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
